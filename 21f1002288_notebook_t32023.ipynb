{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "21f1002288-notebook-t32023",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaibhav1234-pixel/taxi-price-prediction/blob/main/21f1002288_notebook_t32023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "DUHdn0zaeYJO"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "taxi_fare_guru_total_amount_prediction_challenge_path = kagglehub.competition_download('taxi-fare-guru-total-amount-prediction-challenge')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "E2MWF_r4eYJS"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2023-10-28T10:14:37.814001Z",
          "iopub.execute_input": "2023-10-28T10:14:37.814388Z",
          "iopub.status.idle": "2023-10-28T10:14:37.822361Z",
          "shell.execute_reply.started": "2023-10-28T10:14:37.814357Z",
          "shell.execute_reply": "2023-10-28T10:14:37.820375Z"
        },
        "trusted": true,
        "id": "hsTwRG9_eYJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading and Overview of Data"
      ],
      "metadata": {
        "id": "ip33q8OgeYJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Loading data\n",
        "train = pd.read_csv('/kaggle/input/taxi-fare-guru-total-amount-prediction-challenge/train.csv')\n",
        "test = pd.read_csv('/kaggle/input/taxi-fare-guru-total-amount-prediction-challenge/test.csv')\n",
        "\n",
        "# # Taking a look on data\n",
        "# train.describe()\n",
        "train.info()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-28T10:14:37.824534Z",
          "iopub.execute_input": "2023-10-28T10:14:37.824871Z",
          "iopub.status.idle": "2023-10-28T10:14:38.279341Z",
          "shell.execute_reply.started": "2023-10-28T10:14:37.824837Z",
          "shell.execute_reply": "2023-10-28T10:14:38.278729Z"
        },
        "trusted": true,
        "id": "oY4NSjA8eYJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# datetime feature engineering\n",
        "\n",
        "# converted into datetime format\n",
        "train['tpep_pickup_datetime'] = pd.to_datetime(train.tpep_pickup_datetime)\n",
        "train['tpep_dropoff_datetime'] = pd.to_datetime(train.tpep_dropoff_datetime)\n",
        "\n",
        "# swaped pickup datetime with dropoff datetime if pickup datetime is greater than dropoff datetime\n",
        "train[['tpep_pickup_datetime','tpep_dropoff_datetime']] = train[['tpep_pickup_datetime','tpep_dropoff_datetime']].mask( train['tpep_dropoff_datetime'] < train['tpep_pickup_datetime'], train[['tpep_dropoff_datetime','tpep_pickup_datetime']].values)\n",
        "\n",
        "# created some usefull features out of date time features\n",
        "train.loc[:, 'dropoff_hour'] = train['tpep_dropoff_datetime'].dt.hour\n",
        "train.loc[:, 'pickup_hour'] = train['tpep_pickup_datetime'].dt.hour\n",
        "train.loc[:, 'pickup_day'] = train['tpep_pickup_datetime'].dt.dayofweek\n",
        "\n",
        "train.loc[:, 'pickup_weekday'] = train['tpep_pickup_datetime'].dt.day_name()\n",
        "train.loc[:, 'pickup_date'] = train['tpep_pickup_datetime'].dt.day\n",
        "train.loc[:, 'pickup_month'] = train['tpep_pickup_datetime'].dt.month\n",
        "\n",
        "# trip duration in minutes\n",
        "train['ride_duration'] = train['tpep_dropoff_datetime'] - train['tpep_pickup_datetime']\n",
        "train['trip_duration'] = train['ride_duration'].dt.total_seconds()/60\n",
        "\n",
        "train = train.drop(columns=['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'ride_duration'], axis=1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-28T10:14:38.28015Z",
          "iopub.execute_input": "2023-10-28T10:14:38.280576Z",
          "iopub.status.idle": "2023-10-28T10:14:38.483593Z",
          "shell.execute_reply.started": "2023-10-28T10:14:38.280554Z",
          "shell.execute_reply": "2023-10-28T10:14:38.482636Z"
        },
        "trusted": true,
        "id": "1d86wDDOeYJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "X_prY5creYJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, OrdinalEncoder\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "\n",
        "# # removed logging features\n",
        "train = train.drop(columns=['VendorID','payment_type'], axis=1)\n",
        "\n",
        "# # Replaced unusual values with null values to impute later\n",
        "train['RatecodeID'] = train['RatecodeID'].replace(99.0, np.nan)\n",
        "\n",
        "# # Replaced null values of categorical feature with most frequent value (Used Mode Strategy. KNN Imputation was computationaly expensive)\n",
        "train['store_and_fwd_flag'] = train['store_and_fwd_flag'].replace(np.nan, 'N')\n",
        "\n",
        "columns_to_impute = ['passenger_count','RatecodeID','congestion_surcharge','Airport_fee','trip_duration']\n",
        "# columns_to_scale = ['trip_distance','trip_duration']\n",
        "\n",
        "imputer = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='mean'))\n",
        "])\n",
        "\n",
        "# scaler = Pipeline([\n",
        "#     ('scaler', MinMaxScaler())\n",
        "# ])\n",
        "\n",
        "train[columns_to_impute] = imputer.fit_transform(train[columns_to_impute] )\n",
        "# train[columns_to_scale] = scaler.fit_transform(train[columns_to_scale] )\n",
        "# scaling the data affects test score potential reasons can be overfitting or loss of information\n",
        "\n",
        "\n",
        "ct = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('onehot', OneHotEncoder(), ['store_and_fwd_flag'])\n",
        "    ]\n",
        ")\n",
        "\n",
        "train_prepared = ct.fit_transform(train)\n",
        "column_names = ct.get_feature_names_out()\n",
        "df = pd.DataFrame(train_prepared, columns = column_names )\n",
        "\n",
        "train['onehot__store_and_fwd_flag_N'] = df['onehot__store_and_fwd_flag_N']\n",
        "train['onehot__store_and_fwd_flag_Y'] = df['onehot__store_and_fwd_flag_Y']\n",
        "train = train.drop(columns=['store_and_fwd_flag'], axis=1)\n",
        "train.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-28T10:14:38.485574Z",
          "iopub.execute_input": "2023-10-28T10:14:38.485827Z",
          "iopub.status.idle": "2023-10-28T10:14:38.607602Z",
          "shell.execute_reply.started": "2023-10-28T10:14:38.485805Z",
          "shell.execute_reply": "2023-10-28T10:14:38.606585Z"
        },
        "trusted": true,
        "id": "0Z7IhkppeYJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering Test Set\n",
        "\n",
        "# datetime\n",
        "test['tpep_pickup_datetime'] = pd.to_datetime(test.tpep_pickup_datetime)\n",
        "test['tpep_dropoff_datetime'] = pd.to_datetime(test.tpep_dropoff_datetime)\n",
        "\n",
        "test[['tpep_pickup_datetime','tpep_dropoff_datetime']] = test[['tpep_pickup_datetime','tpep_dropoff_datetime']].mask( test['tpep_dropoff_datetime'] < test['tpep_pickup_datetime'], test[['tpep_dropoff_datetime','tpep_pickup_datetime']].values)\n",
        "\n",
        "test.loc[:, 'dropoff_hour'] = test['tpep_dropoff_datetime'].dt.hour\n",
        "\n",
        "test.loc[:, 'pickup_hour'] = test['tpep_pickup_datetime'].dt.hour\n",
        "\n",
        "test.loc[:, 'pickup_day'] = test['tpep_pickup_datetime'].dt.dayofweek\n",
        "# test.loc[:, 'pickup_date'] = test['tpep_pickup_datetime'].dt.day\n",
        "# test.loc[:, 'pickup_month'] = test['tpep_pickup_datetime'].dt.month\n",
        "# train.loc[:, 'pickup_weekday'] = train['tpep_pickup_datetime'].dt.day_name()\n",
        "\n",
        "test['ride_duration'] = test['tpep_dropoff_datetime'] - test['tpep_pickup_datetime']\n",
        "test['trip_duration'] = test['ride_duration'].dt.total_seconds()/60\n",
        "\n",
        "\n",
        "test = test.drop(columns=['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'ride_duration'], axis=1)\n",
        "\n",
        "# Feature Engineering\n",
        "\n",
        "test = test.drop(columns=['VendorID','PULocationID', 'DOLocationID','payment_type'], axis=1)\n",
        "\n",
        "test['RatecodeID'] = test['RatecodeID'].replace(99.0, np.nan)\n",
        "\n",
        "test['store_and_fwd_flag'] = test['store_and_fwd_flag'].replace(np.nan, 'N')\n",
        "\n",
        "columns_to_impute = ['passenger_count','RatecodeID','congestion_surcharge','Airport_fee',]\n",
        "# columns_to_scale = ['trip_distance','trip_duration']\n",
        "\n",
        "scaler = Pipeline([\n",
        "    ('scaler', MinMaxScaler())\n",
        "])\n",
        "\n",
        "imputer = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='mean'))\n",
        "])\n",
        "\n",
        "test[columns_to_impute] = imputer.fit_transform(test[columns_to_impute] )\n",
        "# test[columns_to_scale] = scaler.fit_transform(test[columns_to_scale] )\n",
        "\n",
        "\n",
        "ct = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('onehot', OneHotEncoder(), ['store_and_fwd_flag'])\n",
        "    ]\n",
        ")\n",
        "\n",
        "test_prepared = ct.fit_transform(test)\n",
        "column_names = ct.get_feature_names_out()\n",
        "test_df = pd.DataFrame(test_prepared, columns = column_names )\n",
        "\n",
        "test['onehot__store_and_fwd_flag_N'] = test_df['onehot__store_and_fwd_flag_N']\n",
        "test['onehot__store_and_fwd_flag_Y'] = test_df['onehot__store_and_fwd_flag_Y']\n",
        "\n",
        "test = test.drop(columns=['store_and_fwd_flag'], axis=1)\n",
        "test.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-28T10:14:38.609453Z",
          "iopub.execute_input": "2023-10-28T10:14:38.610613Z",
          "iopub.status.idle": "2023-10-28T10:14:38.713401Z",
          "shell.execute_reply.started": "2023-10-28T10:14:38.610573Z",
          "shell.execute_reply": "2023-10-28T10:14:38.712011Z"
        },
        "trusted": true,
        "id": "5E_yB-WYeYJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA\n",
        "#### Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "IBGN6SAQeYJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# # Some more Datetime features to test\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=(22, 8))\n",
        "\n",
        "# # Dropoff Hour\n",
        "sns.barplot(y=train['passenger_count'], x=train['dropoff_hour'])\n",
        "plt.xlabel('Hour of Day')\n",
        "plt.ylabel('Passenger count')\n",
        "plt.title (\"Passenger count vs Hour of Day\")\n",
        "\n",
        "# # Pickup Hour\n",
        "\n",
        "sns.barplot(y=train['passenger_count'], x=train['pickup_hour'])\n",
        "plt.xlabel('Hour of Day')\n",
        "plt.ylabel('Passenger count')\n",
        "plt.title (\"Passenger count vs Hour of Day\")\n",
        "\n",
        "# Day of Month\n",
        "sns.barplot(y=train['passenger_count'], x=train['pickup_month'])\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Passenger count')\n",
        "plt.title (\"Passenger count vs Month\")\n",
        "\n",
        "# # Date\n",
        "sns.barplot(y=train['passenger_count'], x=train['pickup_date'])\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Passenger count')\n",
        "plt.title (\"Passenger count vs Date\")\n",
        "\n",
        "# # WeekDay\n",
        "sns.barplot(y=train['passenger_count'], x=train['pickup_weekday'])\n",
        "plt.xlabel('Days of week')\n",
        "plt.ylabel('Passenger count')\n",
        "plt.title (\"Passenger count vs Days of week\")\n",
        "\n",
        "# # # these datetime features do not posses enough data\n",
        "train = train.drop(columns=['pickup_weekday','pickup_date', 'pickup_month'], axis=1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-28T10:14:38.7146Z",
          "iopub.execute_input": "2023-10-28T10:14:38.714915Z",
          "iopub.status.idle": "2023-10-28T10:14:38.729574Z",
          "shell.execute_reply.started": "2023-10-28T10:14:38.714886Z",
          "shell.execute_reply": "2023-10-28T10:14:38.728078Z"
        },
        "trusted": true,
        "id": "56QWGILkeYJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "train.hist(bins=50, figsize=(20, 15))\n",
        "\n",
        "# locations are not affecting the total amount\n",
        "train.plot(kind='scatter', x='PULocationID', y='DOLocationID', alpha=0.4, s=train[\"passenger_count\"], label=\"passenger_count\", figsize=(10,7), c='total_amount', cmap=plt.get_cmap(\"jet\"), colorbar=True)\n",
        "plt.legend()\n",
        "\n",
        "# corelation matrix to understand relation between features and target variable\n",
        "corr_matrix = train.corr()\n",
        "corr_matrix[\"total_amount\"].sort_values(ascending=False)\n",
        "plt.figure(figsize=(12,8))\n",
        "sns.heatmap(corr_matrix, annot=True)\n",
        "\n",
        "# scatter plots of important features\n",
        "from pandas.plotting import scatter_matrix\n",
        "attributes = ['total_amount','extra', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'Airport_fee']\n",
        "scatter_matrix(train[attributes], figsize=(12,8))\n",
        "\n",
        "# magnifying important visuals from last plot\n",
        "train.plot(kind='scatter', x='total_amount', y='tolls_amount', alpha=0.4) #  has some outliers and horizontal lines\n",
        "train.plot(kind='scatter', x='total_amount', y='tip_amount', alpha=0.4) # there is some scatter at extremes\n",
        "train.plot(kind='scatter', x='total_amount', y='DOLocationID', alpha=0.4) # does not affect total_amounts though have little scatter at right but seems uniform\n",
        "train.plot(kind='scatter', x='total_amount', y='PULocationID', alpha=0.4) # same as DOLocationID\n",
        "\n",
        "\n",
        "# # # Removed features that do not affect target variable\n",
        "train = train.drop(columns=['PULocationID', 'DOLocationID'], axis=1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-28T10:14:38.731927Z",
          "iopub.execute_input": "2023-10-28T10:14:38.732398Z",
          "iopub.status.idle": "2023-10-28T10:14:38.754632Z",
          "shell.execute_reply.started": "2023-10-28T10:14:38.73236Z",
          "shell.execute_reply": "2023-10-28T10:14:38.753225Z"
        },
        "trusted": true,
        "id": "aRevPcNoeYJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # # dropping outliers worsen the result on test set\n",
        "# train = train[train['total_amount'] >= 200]\n",
        "# train = train[train['total_amount'] <= 500]\n",
        "# target = train['total_amount']\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.boxplot(train['tip_amount'])\n",
        "plt.title(\"Boxplot of Tip Variable\")\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.boxplot(train['extra'])\n",
        "plt.title(\"Boxplot of Extra Variable\")\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.boxplot(train['tolls_amount'])\n",
        "plt.title(\"Boxplot of Extra Variable\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-28T10:14:38.756113Z",
          "iopub.execute_input": "2023-10-28T10:14:38.756455Z",
          "iopub.status.idle": "2023-10-28T10:14:39.262129Z",
          "shell.execute_reply.started": "2023-10-28T10:14:38.756429Z",
          "shell.execute_reply": "2023-10-28T10:14:39.261133Z"
        },
        "trusted": true,
        "id": "wH4BljKjeYJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Selecting Model and Tuning Hyperparameters"
      ],
      "metadata": {
        "id": "pZ2TkayceYJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from numpy import arange\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, VotingRegressor\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "# with cross validation\n",
        "X = train.drop('total_amount', axis=1)\n",
        "y = train['total_amount']\n",
        "\n",
        "# # with validation set\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# hyperparameter tuning for ExtraTreesRegressor and RandomForestRegressor is done manually because of computational expense and time taking was in hours for given hardware. these are values tested -->\n",
        "# n_estimators = [100, 200, 300, 400]\n",
        "# max_depth = [10, 20, 30]\n",
        "# min_samples_split = [2, 5, 7, 8, 10]\n",
        "# min_samples_leaf = [1, 2]\n",
        "\n",
        "# # Starting with simple regression model\n",
        "# lin_reg = LinearRegression() # does not perform well\n",
        "\n",
        "# dtr = DecisionTreeRegressor(max_depth=30, random_state= 42, min_samples_split=7) # 0.9299 on validation set\n",
        "\n",
        "\n",
        "# # Second Best Model\n",
        "# rfr = RandomForestRegressor(n_estimators=300, max_depth=30, random_state= 42, min_samples_split=7) # 0.9604 on validation set\n",
        "\n",
        "# # BEST MODEL\n",
        "etr = ExtraTreesRegressor(n_estimators=300,max_depth=30, random_state= 42, min_samples_split=7) # 0.9641 on validation set\n",
        "\n",
        "\n",
        "\n",
        "# knn = KNeighborsRegressor(n_neighbors=7) # 0.8855 on validation set\n",
        "\n",
        "# # hyperparameter search for knn regressor\n",
        "\n",
        "# knn_param_grid = {\n",
        "#     'n_neighbors': [3, 5, 7, 9, 11]  # You can customize this list\n",
        "# }\n",
        "\n",
        "# grid_search = GridSearchCV(estimator=knn, param_grid=knn_param_grid, cv=10, scoring='r2')\n",
        "\n",
        "# grid_search.fit(X, y)\n",
        "# best_n_neighbors = grid_search.best_params_['n_neighbors']\n",
        "# print(\"Best number of neighbors:\", best_n_neighbors)\n",
        "\n",
        "# # Here are some other regressor used\n",
        "# xgb = xgb.XGBRegressor(\n",
        "#     n_estimators=200,  # Number of boosting rounds\n",
        "#     learning_rate=0.1,  # Step size shrinkage\n",
        "#     max_depth=20,  # Maximum depth of individual trees\n",
        "#     objective='reg:squarederror'  # Objective function for regression\n",
        "# ) # 0.94 on validation set (approx.)\n",
        "\n",
        "# # LGB is fastest model with 0.93 r2 score on test\n",
        "# lgb = lgb.LGBMRegressor(\n",
        "#     n_estimators=400,\n",
        "#     learning_rate=0.1,\n",
        "#     max_depth=30,\n",
        "# )\n",
        "\n",
        "# gradient_boost = GradientBoostingRegressor(\n",
        "#     n_estimators=300,\n",
        "#     learning_rate=0.01,\n",
        "#     max_depth=10,\n",
        "#     random_state=42\n",
        "# ) # 0.95 on validation set (approx.)\n",
        "\n",
        "# vr = VotingRegressor(estimators=[\n",
        "#     ('xgboost', xgboost),\n",
        "#     ('lgbm', lgbm),\n",
        "#     ('gradient_boost', gradient_boost)\n",
        "# ]) # 0.95 on validation set (approx.)\n",
        "\n",
        "\n",
        "# # SVM is most expensive out of all models\n",
        "# svr = SVR(kernel='linear', C=1.0, epsilon=0.2) # 0.89 (approx.) on validation set\n",
        "\n",
        "# bagging = BaggingRegressor(dtr, n_estimators=300, random_state=42) # this model is roughly equivalent to random forest so it performs same 0.9603 r2 score\n",
        "\n",
        "\n",
        "# # Checking Scores with\n",
        "# scores = cross_val_score(etr, X, y, cv=5, scoring='r2')\n",
        "# scores.mean()\n",
        "\n",
        "# # or\n",
        "\n",
        "# etr.fit(X_train,y_train)\n",
        "# pred = etr.predict(X_val)\n",
        "# r2 = r2_score(y_val,pred)\n",
        "# r2\n",
        "\n",
        "\n",
        "etr.fit(X,y)\n",
        "\n",
        "pred = etr.predict(test)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-28T10:14:39.265165Z",
          "iopub.execute_input": "2023-10-28T10:14:39.266952Z",
          "iopub.status.idle": "2023-10-28T10:14:43.874555Z",
          "shell.execute_reply.started": "2023-10-28T10:14:39.266921Z",
          "shell.execute_reply": "2023-10-28T10:14:43.873631Z"
        },
        "trusted": true,
        "id": "Rcm1omQSeYJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# File Submission"
      ],
      "metadata": {
        "id": "I9GKOPdeeYJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "submission = pd.DataFrame(columns=['ID', 'total_amount'])\n",
        "\n",
        "submission['ID'] = [i for i in range(1, len(pred)+1)]\n",
        "submission['total_amount'] = pred\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "submission.head()\n",
        "submission.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-28T10:14:43.875902Z",
          "iopub.execute_input": "2023-10-28T10:14:43.876237Z",
          "iopub.status.idle": "2023-10-28T10:14:43.960849Z",
          "shell.execute_reply.started": "2023-10-28T10:14:43.876208Z",
          "shell.execute_reply": "2023-10-28T10:14:43.959887Z"
        },
        "trusted": true,
        "id": "MqUlS_egeYJb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}